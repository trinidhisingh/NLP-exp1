{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73e82ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', ',', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
      "\n",
      "Split function Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for,', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Word Tokenization\n",
    "text = \"NLTK is a leading platform for, building Python programs to work with human language data.\"\n",
    "tokens_word = word_tokenize(text)\n",
    "print(\"Word Tokenization:\")\n",
    "print(tokens_word)\n",
    "\n",
    "# Split function tokenization\n",
    "text = \"NLTK is a leading platform for, building Python programs to work with human language data.\"\n",
    "tokens_split = text.split()\n",
    "print(\"\\nSplit function Tokenization:\")\n",
    "print(tokens_split)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bb5c73",
   "metadata": {},
   "source": [
    "# Difference btw Word Tokenization and Sentence Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb87ae11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
      "\n",
      "Sentence Tokenization:\n",
      "['NLTK is a leading platform for building Python programs.', \"It's widely used for natural language processing tasks.\"]\n",
      "\n",
      "Regular Expression Tokenization:\n",
      "['john.doe@example.com', 'http://www.example.com']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Word Tokenization\n",
    "text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "tokens_word = word_tokenize(text)\n",
    "print(\"Word Tokenization:\")\n",
    "print(tokens_word)\n",
    "\n",
    "# Sentence Tokenization\n",
    "text = \"NLTK is a leading platform for building Python programs. It's widely used for natural language processing tasks.\"\n",
    "tokens_sent = sent_tokenize(text)\n",
    "print(\"\\nSentence Tokenization:\")\n",
    "print(tokens_sent)\n",
    "\n",
    "# Regular Expression Tokenization\n",
    "text = \"Email me at john.doe@example.com or visit http://www.example.com for more information.\"\n",
    "\n",
    "# Define a regular expression pattern to tokenize email addresses and URLs\n",
    "pattern = r'\\b(?:[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b|https?://\\S+)\\b'\n",
    "tokens_regex = regexp_tokenize(text, pattern)\n",
    "print(\"\\nRegular Expression Tokenization:\")\n",
    "print(tokens_regex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130c4c6",
   "metadata": {},
   "source": [
    "# Difference btw Word Tokenization and Punkt Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f12c3f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization using NLTK's PunktTokenizer:\n",
      "['NLTK is a leading platform for building Python programs.', \"It's widely used for natural language processing tasks.\"]\n",
      "\n",
      "Word Tokenization using NLTK's word_tokenize:\n",
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', '.', 'It', \"'s\", 'widely', 'used', 'for', 'natural', 'language', 'processing', 'tasks', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rohit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, PunktSentenceTokenizer\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sentence Tokenization using NLTK's PunktTokenizer\n",
    "punkt_tokenizer = nltk.PunktSentenceTokenizer()\n",
    "text = \"NLTK is a leading platform for building Python programs. It's widely used for natural language processing tasks.\"\n",
    "tokens_sent_punkt = punkt_tokenizer.tokenize(text)\n",
    "print(\"Sentence Tokenization using NLTK's PunktTokenizer:\")\n",
    "print(tokens_sent_punkt)\n",
    "\n",
    "# Word Tokenization using NLTK's word_tokenize\n",
    "tokens_word = []\n",
    "for sent in tokens_sent_punkt:\n",
    "    tokens_word.extend(word_tokenize(sent))\n",
    "\n",
    "print(\"\\nWord Tokenization using NLTK's word_tokenize:\")\n",
    "print(tokens_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b796d7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fa5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575d5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b6c35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2a858e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
